{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datngominh/sp-converter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-24 21:24:16,695\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 6024.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /home/datngominh/sp-converter/models/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85\n",
      "INFO 03-24 21:24:19 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-24 21:24:19 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-24 21:24:25 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 03-24 21:24:25 config.py:1129] Possibly too large swap space. 4.00 GiB out of the 7.41 GiB total CPU memory is allocated for the swap space.\n",
      "INFO 03-24 21:24:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/datngominh/sp-converter/models/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85', speculative_config=None, tokenizer='/home/datngominh/sp-converter/models/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/datngominh/sp-converter/models/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-24 21:24:26 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-24 21:24:26 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-24 21:24:27 model_runner.py:1110] Starting to load model /home/datngominh/sp-converter/models/models--microsoft--phi-3-mini-4k-instruct/snapshots/0a67737cc96d2554230f90338b163bc6380a2a85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.58s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  3.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.02s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 21:24:36 model_runner.py:1115] Loading model weights took 7.1184 GB\n",
      "INFO 03-24 21:24:39 worker.py:267] Memory profiling takes 2.45 seconds\n",
      "INFO 03-24 21:24:39 worker.py:267] the current vLLM instance can use total_gpu_memory (12.00GiB) x gpu_memory_utilization (0.90) = 10.80GiB\n",
      "INFO 03-24 21:24:39 worker.py:267] model weights take 7.12GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.32GiB; the rest of the memory reserved for KV Cache is 3.33GiB.\n",
      "INFO 03-24 21:24:39 executor_base.py:111] # cuda blocks: 567, # CPU blocks: 682\n",
      "INFO 03-24 21:24:39 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 2.21x\n",
      "INFO 03-24 21:24:41 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:24<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-24 21:25:06 model_runner.py:1562] Graph capturing finished in 28 secs, took 0.24 GiB\n",
      "INFO 03-24 21:25:06 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 30.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pull and load model \n",
    "from components.rag_pipeline import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the application...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.53s/it, est. speed input: 80.59 toks/s, output: 32.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Test complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the application\n",
    "print(\"Testing the application...\")\n",
    "\n",
    "response = graph.invoke({\"question\": \"\"\"\n",
    "    SELECT [AddressID]\n",
    "      ,[AddressLine1]\n",
    "      ,[AddressLine2]\n",
    "      ,[City]\n",
    "      ,[StateProvinceID]\n",
    "      ,[PostalCode]\n",
    "      ,[SpatialLocation]\n",
    "      ,[rowguid]\n",
    "      ,[ModifiedDate]\n",
    "  FROM [Person].[Address]\n",
    "\"\"\"})\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "# Write the answer to a text file\n",
    "with open(\"response_answer.txt\", \"w\") as file:\n",
    "    file.write(response[\"answer\"])\n",
    "\n",
    "print(\"Test complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
